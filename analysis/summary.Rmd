---
title: "Prag Bat Summary"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F)

library(tidyverse)
library(knitr)
library(ggthemes)
library(jsonlite)
library(readxl)
library(corrr)
library(corrplot)
library(ggcorrplot)
library(reshape2)
#library(psych)
library(readODS)
#library(brms)
library(psy)
library(tidyboot)
library(lubridate)
library(ggpubr)
library(lavaan)
library(blavaan)
library(tidySEM)
library(semPlot)
```

```{r, include=FALSE}
data <- bind_rows(
  read_csv("../data/data_r1.csv") %>%filter(task != "training") %>%mutate(study = "S1"),
  read_csv("../data/data_r2.csv") %>%mutate(study = "S2")
  ) %>%
  mutate(study = factor(study),
         id = paste(study, id, sep = "_"))
  
```

```{r, include=FALSE}
data %>%
  group_by(subage) %>%
  summarise(n= length(unique(id)))
```
# Objective

Assess reliability of and relations between different pragmatic inference tasks which we have developed over the years.

# Study 1 and 2

## Sample size

Our goal was to test at least 20 children per age group and round twice. Children were tested in a kindergarten in Leipzig.

```{r}
data %>%
  group_by(subage,id,study) %>%
  summarise(testdays = length(unique(test_day))) %>%
  group_by(study,subage) %>%
  summarise(n = length(unique(id)),
            complete_retest_data = sum(testdays == 2)) %>%
  kable()
```

## Results by Task

All tasks had 5 trials, except for card sorting, which had 6. All subjects got the tasks in the same order and the same version of each task.

Study 1 included the following tasks: 

* Informativeness (same setup and stimuli as in MCC)
* Preference (as in MCC)
* Novelty (as in SPIN)
* Mutual exclusivity (as in SPIN, with fewer items)
* Card sorting (DCCS sensu Zelazo, 2006)

Study 2 included the following tasks: 

* Simple informativeness (same setup and stimuli as in Frank & Goodman, 2014)
* Ad-hoc implicature (same stimuli as Yoon & Frank, 2019)
* Discourse continuity (as in DISCON)
* Mutual exclusivity (as in SPIN, with fewer items, same as R1)
* Card sorting (DCCS sensu Zelazo, 2006, same as R1)

For mutual exclusivity and card sorting we have a direct replication. The plot below shows all data, including data from children who were tested only once.

```{r}
p1 <- data %>%
  group_by(study, id,age, subage,task) %>%
  filter(task != "training") %>%
  summarise(mean = mean(correct))

p2 <- p1 %>%
  group_by(study,subage,task) %>%
  tidyboot_mean(column = mean) %>%
  mutate(chance = ifelse(task == "discourse_continuity", 1/3, 1/2))

ggplot()+
  geom_hline(data = p2, aes(yintercept = chance), lty = 2)+
  geom_smooth(data = p1, aes(x = age, y = mean, lty = study), method = "lm", col = "black", size = 1)+
  geom_jitter(data = p1, aes(x = age, y = mean, pch = study), alpha = .5, width = .05, height = .01)+
  geom_pointrange(data = p2, aes(x = as.numeric(as.character(subage))+.5, y = mean, ymin = ci_lower, ymax = ci_upper, col = factor(subage), pch = study), position = position_dodge(width = .5))+
  facet_wrap(~task)+
  labs(x = "Age Group", y = "Proportion Correct") +
  scale_color_ptol(name = "Age") +
  ylim(-0.05, 1.05)+
  theme_few()+
  theme(legend.position = c(.85,.1), legend.direction = "horizontal")

```

## Reliability

Based on simple Pearson correlations on the data aggregated by subject, task and test day.

```{r}
wide_data <- data %>%
  filter(task != "training") %>%
  droplevels() %>%
  group_by(study,id,task, test_day) %>%
  summarise(mean = mean(correct)) %>%
  spread(test_day, mean) %>%
  na.omit() %>%
  rename("Day1" = `1`,
         "Day2" = `2`) 

reli <- wide_data %>%
  group_by(study, task) %>%
  summarize(reli = cor.test(Day1,Day2)$estimate,
            lci = cor.test(Day1,Day2)$conf.int[1],
            uci = cor.test(Day1,Day2)$conf.int[2],
            p = cor.test(Day1,Day2)$p.value, 
            n = n()) %>%
  mutate_if(is.numeric, round, digits = 2)

reli %>%
  kable()
```
Explore consolidating across studies. 

```{r}
reli_task <- wide_data %>%
  group_by(task) %>%
  summarize(reli = cor.test(Day1,Day2)$estimate,
            lci = cor.test(Day1,Day2)$conf.int[1],
            uci = cor.test(Day1,Day2)$conf.int[2],
            p = cor.test(Day1,Day2)$p.value, 
            n = n()) %>%
  mutate_if(is.numeric, round, digits = 2)

reli_task %>%
  arrange(desc(reli)) %>%
  kable()
```

## Correlations between tasks

Based on simple Pearson correlations on the data aggregated by subject and task.

```{r, fig.align="center"}
cor_r1 <- data %>%
  filter(study == "S1") %>%
  droplevels() %>%
  group_by(id,task) %>%
  summarise(mean = mean(correct)) %>%
  spread(task, mean) %>%
  ungroup() %>%
  select(-id) %>%
  corrr::correlate(diagonal = reli %>%
                     filter(study == "S1") %>%
                     pull(reli)) %>%
  gather(task, cor, -term) %>%
  mutate(cor = replace(cor, duplicated(cor), NA)) %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  na.omit() %>%
  mutate(study = "S1")

cor_r2 <- data%>%
  filter(study == "S2") %>%
  droplevels() %>%
  group_by(id,task) %>%
  summarise(mean = mean(correct)) %>%
  spread(task, mean) %>%
  ungroup() %>%
  select(-id) %>%
  corrr::correlate(diagonal = reli%>%filter(study == "S2") %>%pull(reli)) %>%
  gather(task, cor, -term) %>%
  mutate(cor = replace(cor, duplicated(cor), NA)) %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  na.omit() %>%
  mutate(study = "S2")
  
cor <- bind_rows(
  cor_r1,
  cor_r2
)

ggarrange(
ggplot(cor_r1, aes(x = term, y = task, fill = cor))+
  geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation") +
   coord_fixed()+
  theme_few()+
  geom_text(aes(label = cor), color = "black", size = 3) +
  ggtitle("Study 1")+
  theme(
        legend.position = "right",
        legend.direction = "horizontal",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)),


ggplot(cor_r2, aes(x = term, y = task, fill = cor))+
  geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation") +
   coord_fixed()+
  theme_few()+
  geom_text(aes(label = cor), color = "black", size = 3) +
  ggtitle("Study 2")+
  theme(
        legend.position = "right",
        legend.direction = "horizontal",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)),
common.legend = T, legend = "top"

)
```

# Study 3

The goal of Study 3 was to select some tasks from Study 1 and 2 and collect data from a larger sample to look more closely at the relations between tasks. We aimed for 120 children (60 3yo, 60 4yo).

Study 3 included the following tasks: 

* Simple informativeness (same setup and stimuli as in Frank & Goodman, 2014, same as S2)
* Ad-hoc implicature (same stimuli as Yoon & Frank, 2019, same as S2)
* Mutual exclusivity (as in SPIN, with fewer items, same as S1 and S2)
* Card sorting (DCCS sensu Zelazo, 2006, same as S1 and S2)
* Match to sample (setup and stimuli from Christie & Gentner, 2013)

In contrast to Study 1 and 2, we tested participants only once, with 5 or 6 trials per task. 

## Sample size
```{r}
data3 <- read_csv("../data/data_r3.csv")
```

```{r}
data3%>%
  group_by(subage)%>%
  summarise(n= length(unique(id)))%>%
  kable()
```


## Results by task 
```{r}
p31 <- data3%>%
  group_by(id,age, subage,task)%>%
  filter(task != "training")%>%
  summarise(mean = mean(correct))

p32 <- p31 %>%
  group_by(subage,task)%>%
  tidyboot_mean(column = mean)%>%
  mutate(chance = 1/2)

ggplot()+
  geom_hline(data = p32, aes(yintercept = chance), lty = 2)+
  geom_smooth(data = p31, aes(x = age, y = mean), method = "lm", col = "black", size = 1)+
  geom_point(data = p31, aes(x = age, y = mean), alpha = .5, width = .05, height = .01)+
  geom_pointrange(data = p32, aes(x = as.numeric(as.character(subage))+.5, y = mean, ymin = ci_lower, ymax = ci_upper, col = factor(subage)))+
  facet_wrap(~task)+
  labs(x = "Age Group", y = "Proportion Correct")+
  scale_color_ptol(name = "Age")+
  ylim(-0.05, 1.05)+
  xlim(3,5)+
  theme_few()+
  theme(legend.position = c(.85,.2), legend.direction = "horizontal")
```
Results replicate the previous studies. The newly added match-to-sample task did not work too well in that we have no solid group-level performance above chance for the two age groups and no age effect. Reliability for the task is also unknown as it was not part of the previous studies. 

## Correlations between tasks

```{r}
cor3 <- data3%>%
  filter(task != "training")%>%
  droplevels()%>%
  group_by(id,task)%>%
  summarise(mean = mean(correct))%>%
  spread(task, mean)%>%
  ungroup()%>%
  select(-id)%>%
  corrr::correlate()%>%
  gather(task, cor, -term)%>%
  mutate(cor = replace(cor, duplicated(cor), NA))%>%
  mutate_if(is.numeric, round, digits = 2)%>%
  na.omit()
  

ggplot(cor3, aes(x = term, y = task, fill = cor))+
  geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation") +
   coord_fixed()+
  theme_few(base_size = 12)+
  geom_text(aes(label = cor), color = "black", size = 3) +
  theme(legend.justification = c(1, 0),
        legend.position = c(0.55, 0.75),
        legend.direction = "horizontal",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), 
        legend.background = element_blank())+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                title.position = "top", title.hjust = 0.5))
```

Correlations between tasks are lower compared to Study 1 and 2. This was expected since we only have half the number of trials in this study (Study 1 and 2 both had two test days, here we only had 1). Nevertheless, the overall pattern seems similar to study 2. 

## Factor analysis

Given the low number of tasks, exploratory factor analysis does not make much sense because a one factor model will always be favored. Instead, I fit the CFA model that I thought best represents our theoretical view of the data. That is, a pragmatics factor with the three tasks as indicators, which is then correlated with the match-to-sample and the card sorting task: 

`prag  =~ ad_hoc_implicature + mutual_exclusivity + simple_inf`

`card  =~ card_sorting`

`match =~ match_to_sample`

Figure below shows the result from the frequentist model. A Bayesian version produces essentially the same results (but takes forever to run and is more difficult to visualize). All fit indices for the frequentist and the Bayesian model show a good fit (see below for frequentist model - hints for interpretation: https://easystats.github.io/effectsize/reference/interpret_gfi.html). 

```{r}
semd <- p31%>%
  pivot_wider(names_from = "task", values_from = "mean")%>%
  ungroup()%>%
  filter(!is.na(ad_hoc_implicature),
         !is.na(card_sorting),
         !is.na(match_to_sample),
         !is.na(mutual_exclusivity),
         !is.na(simple_inf))%>%
  mutate(ad_hoc_implicature = scale(ad_hoc_implicature),
         card_sorting = scale(card_sorting),
         match_to_sample = scale(match_to_sample),
         mutual_exclusivity = scale(mutual_exclusivity),
         simple_inf = scale(simple_inf))

model <- 'prag  =~ ad_hoc_implicature + mutual_exclusivity + simple_inf
          card  =~ card_sorting
          match =~ match_to_sample'

model2 <- 'fac  =~ ad_hoc_implicature + mutual_exclusivity + simple_inf + card_sorting + match_to_sample'

model3 <- 'ad  =~ ad_hoc_implicature
           simp  =~ simple_inf
           mut  =~ mutual_exclusivity
           card  =~ card_sorting
           match =~ match_to_sample'
```

```{r}
ffit <- cfa(model, data = semd)

# ffit2 <- cfa(model2, data = semd)
# 
# ffit3 <- cfa(model3, data = semd)
```

```{r}
#summary(ffit, fit.measures=TRUE, standardized = TRUE, rsquare = T)

fitMeasures(ffit, c("chisq", "df", "pvalue", "cfi", "rmsea", "rmsea.ci.lower", "rmsea.ci.upper", "srmr"))%>%
  as_tibble(rownames = "index")%>%
  kable()
```

```{r}
cfa_scores <- lavPredict(ffit)%>%
  as_tibble(rownames = "id")%>%
  mutate(id = paste("VP",id, sep = ""))
```


Graph below shows standardized model coefficients from frequentist model. As you can see the pragmatics factor is related to the card sorting task but not to the match-to-sample task. 

```{r}
lay <- get_layout("card", "", "prag","","match",
                  "card_sorting", "ad_hoc_implicature", "mutual_exclusivity", "simple_inf", "match_to_sample", rows = 2)

graph_sem(model = ffit, layout = lay)
```

In a Bayesian model comparison via WAIC the model above does better compared to two (not very strong) alternative models: a `single factor model` and a `one factor per task model`. 


```{r}
# fit <- bcfa(model, data = semd, bcontrol = list(cores = 3), target = "stan", n.chains = 3, burnin = 3000, sample = 3000)
# 
# fit2 <- bcfa(model2, data = semd, bcontrol = list(cores = 3), target = "stan", n.chains = 3, burnin = 3000, sample = 3000)
# 
# fit3 <- bcfa(model3, data = semd, bcontrol = list(cores = 3), target = "stan", n.chains = 3, burnin = 3000, sample = 3000)
```

```{r}
# blavCompare(fit, fit2)
# 
# blavCompare(fit, fit3)
# 
# blavInspect(fit, "lvs")
```


```{r}
# summary(fit, ci = T)
# 
# summary(fit, fit.measures=TRUE, standardized = TRUE, rsquare = T)
# 
# semPaths(fit, whatLabels="std", intercepts=FALSE, style="lisrel",
#                        nCharNodes=0, 
#                        nCharEdges=0,
#                        curveAdjacent = TRUE,title=TRUE, layout = lay,node.width = 2)
```

## RSA model

One of the reasons we picked the three pragmatics tasks we did was that we can all model them via RSA. That is, we can estimate a value for $\alpha$ for each participant based on the data from those three tasks. To account for different difficulties, there is an additional scale parameter that adjusts $\alpha$ for each task. In the end, we can treat the participant specific $\alpha$ estimate like a factor score and correlate it with the other two tasks. The results are quite similar to the CFA model above. 

```{r}
rsa_est <- read.csv("../../../Spin-off/spin-off/data/rsa_est.csv")%>%
  mutate(mode = scale(mode))
```

```{r}
p31%>%
  filter(task == "card_sorting" | task == "match_to_sample")%>%
  group_by(task)%>%
  mutate(mean = scale(mean))%>%
  ungroup()%>%
  left_join(rsa_est)%>%
  ggplot(aes(x = mean, y = mode))+
  geom_abline(intercept = 0, slope = 1, lty = 2, alpha = .75)+
  geom_point()+
  geom_smooth(method = "lm", col= "firebrick")+
  stat_cor()+
  labs(x = "mean task performance (scaled)", y = "alpha (RSA) estimate (scaled)")+
  facet_grid(~task)+
  theme_minimal()
```

## Things to discuss

* should we take out the match-to-sample task (unknown reliability, unsure if it "worked") and instead include the data from study 2 in the CFA/RSA model?
* RSA or CFA or both?

