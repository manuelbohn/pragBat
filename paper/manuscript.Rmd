---
title             : "An individual differences perspective on the development of pragmatic abilities in the preschool years"
shorttitle        : "Individual differences in pragmatic abilities"

author: 
  - name          : "Manuel Bohn"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, 04103 Leipzig, Germany"
    email         : "manuel_bohn@eva.mpg.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Methodology
      - Formal Analysis
      - Visualization
      - Writing – original draft
      - Writing – review & editing
  - name          : "Michael Henry Tessler"
    affiliation   : "2,3"
    role:
      - Formal Analysis
      - Writing – review & editing
  - name          : "Clara Kordt"
    affiliation   : "4"
    role:
      - Investigation
      - Writing – review & editing
  - name          : "Tom Hausmann"
    affiliation   : "5"
    role:
      - Investigation
      - Writing – review & editing
  - name          : "Michael C. Frank"
    affiliation   : "6"
    role:
      - Conceptualization
      - Writing – review & editing

affiliation:
  - id            : "1"
    institution   : "Department of Comparative Cultural Psychology, Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany"
  - id            : "2"
    institution   : "DeepMind, London, UK"
  - id            : "3"
    institution   : "Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, USA"
  - id            : "4"
    institution   : "Martin Luther University Halle-Wittenberg, Halle (Saale), Germany"
  - id            : "5"
    institution   : "Brandenburg Medical School Theodor Fontane, Neuruppin, Germany"
  - id            : "6"
    institution   : "Department of Psychology, Stanford University, Stanford, USA"

authornote: |
  We are very thankful to Stella Christie for sharing the material for the relational match-to-sample task with us. M. Bohn received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement no. 749229. M. H. Tessler was funded by the National Science Foundation SBE Postdoctoral Research Fellowship Grant No. 1911790. M. C. Frank was supported by a Jacobs Foundation Advanced Research Fellowship and the Zhou Fund for Language and Cognition. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.

abstract: |
  Pragmatic abilities are fundamental to successful language use and learning. Individual differences studies contribute to understanding the psychological processes involved in pragmatic reasoning. Small sample sizes, insufficient measurement tools, and a lack of theoretical precision have hindered progress, however. Three studies addressed these challenges in three- to five-year-old German-speaking children (N = 228, 121 female). Studies 1 and 2 assessed the psychometric properties of six pragmatics tasks. Study 3 investigated relations among pragmatics tasks and between pragmatics and other cognitive abilities. The tasks were found to measure stable variation between individuals. Via a computational cognitive model, individual differences were traced back to a latent pragmatics construct. This presents the basis for understanding the relations between pragmatics and other cognitive abilities.
  
keywords          : "Pragmatics, language development, individual differences, cognitive modeling"
wordcount         : "8558"

bibliography      : "../../../References/library.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(tidyverse)
library(knitr)


```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r}
data <- bind_rows(
  read_csv("../data/data_r1.csv") %>%filter(task != "training") %>%mutate(study = "study1"),
  read_csv("../data/data_r2.csv") %>%filter(task != "training")%>%mutate(study = "study2"),
  read_csv("../data/data_r3.csv") %>%filter(task != "training")%>%mutate(study = "study3")
  ) %>%
  mutate(study = factor(study),
         id = paste(study, id, sep = "_"), 
         test_day =  factor(test_day))
```
# Introduction 

Communication predates language. Before children produce their first words, they communicate with the world around them using vocalizations and gestures [@bruner1974communication; @bates1979emergence]. The process of language learning recruits many of the social-cognitive processes that underlie pre-verbal communication [@bohn2019pervasive; @tomasello2009constructing; @clark2009first]. Even for proficient language users, communication is not reducible to the words being exchanged. The common thread running through the different aspects of human communication is its inferential nature: what a speaker means -- verbal or otherwise -- is underdetermined by the parts that make up the utterance. It takes contextual social inferences, often referred to as *pragmatic* inferences, to recover the intended meaning [@grice1991studies; @clark1996using; @sperber2001relevance; @levinson2000presumptive; @bohn2018common]. 

The development of pragmatics has been widely studied in recent years [for a recent review see @bohn2019pervasive]. This research covers a range of different phenomena ranging from so-called *pure pragmatics* [@matthews2014pragmatic] in non-verbal communication in infancy to sophisticated linguistic inferences developing much later [@huang2009online; @papafragou2016scalar]. A growing portion of this work is devoted to studying individual differences [@matthews2018individual; @wilson2021pragmatic]. The motivation behind the move to study individual variation is twofold: first, individual differences offer insights into the underlying psychological processes. If two phenomena (e.g. pragmatic reasoning and executive functions) vary together this is consistent with shared cognitive processes [@kidd2018individual; @matthews2018individual; @wilson2022novel], though it is not definitive evidence for such a claim. Second, deficits in pragmatic abilities have been linked to maladaptive behavioral patterns and forms of language impairment [@helland2014stable].

In their recent review, @matthews2018individual identified three issues that significantly limit what we can learn from individual differences research on pragmatic abilities. First, most studies have insufficient sample sizes so that small and medium sized correlations among pragmatics tasks and between pragmatics tasks and measures for other cognitive abilities cannot be reliably detected [mirroring issues in estimating correlations across other fields, @schonbrodt2013sample]. Second, the tasks used to assess pragmatic abilities often have poor or unknown psychometric properties. For example, many tasks only have a single trial and are therefore unable to capture variation between children [see also @enkavi2019large]. Furthermore, reliability is not assessed, making it unclear if the task captures stable characteristics [@russell2008assessing; @flake2020measurement]. Third, the cognitive processes underlying pragmatic inferences in a particular task are underspecified. As a consequence, there is often no clear rationale for why a particular target task should correlate with another cognitive measure.

In search of a better understanding of individual variation in pragmatic ability, the studies presented here directly address these issues. We identified six pragmatic reasoning tasks in children between three and five years of age and investigated their psychometric properties, in particular their re-test reliability. Reliable tasks are a necessary precondition for meaningful individual differences research [@hedge2018reliability; @fried2018measurement]. Next, we investigated the relations among different pragmatic reasoning tasks as well as between pragmatic reasoning and other cognitive abilities in a sample large enough to detect small to medium sized correlations. For this purpose, we introduced computational cognitive models of pragmatic reasoning to the study of individual differences. Computational cognitive models formalize hypotheses about cognitive processes that could underlie pragmatic reasoning; thus, the use of these models provides a substantive theoretical account of why certain pragmatic reasoning tasks should be related to one another. Here, we use the formalism introduced by the Rational Speech Act (RSA) framework [@frank2012predicting; @goodman2016pragmatic]. RSA models see pragmatic inferences as a special case of (Bayesian) social reasoning. A pragmatic listener interprets an utterance by assuming it was produced by a cooperative speaker. The speaker tries to be informative, that is, they provide messages that would increase the probability that the listener will recover their intended meaning. The informativeness of an utterance arises from a contrastive inference in which the effects of multiple – plausible – utterances are compared. We assume that this inference process is shared by some of the pragmatics tasks involved in this study and can thus be used to account for individual differences (see below). 

The six tasks we selected were developmental adaptations of referential communication games inspired by research in experimental pragmatics [@noveck2008experimental; @noveck2004experimental]. They all share a common trial-by-trial structure in which the test event always involved an agent producing an ambiguous utterance that the child had to resolve using pragmatic reasoning. This structure allowed us to run multiple trials per task, increasing reliability. We grouped the tasks into two broad categories (Figure \@ref(fig:fig1)). *Utterance-based tasks* asked children to derive inferences from the words and gestures the speaker produced in context. *Common ground/discourse-based tasks* asked children to derive inferences from the social interaction that preceded the utterance.

For the utterance-based category, we selected mutual exclusivity, informativeness inference, and ad-hoc implicature tasks. "Mutual exclusivity" describes the phenomenon that children tend to map a novel word to an unknown object [@markman1988children; @halberda2003development; @bion2013fast; @merriman1989mutual; @clark1988logic; @lewis2020role]. Following @lewis2020role, we use the term "mutual exclusivity" as a convenient term to denote a specific task. This term is also related to a particular theoretical account of the phenomenon [@markman1990constraints], but we do not presuppose that specific account. Informativeness inferences describe situations in which children identify the referent of a novel word by assuming that the speaker is trying to be informative. Being informative translates to using words that reduce ambiguity and help the listener to recover the intended meaning [@frank2014inferring]. Ad-hoc implicature describes inferences that ask the child to contrast an utterance with alternatives that the speaker could have used but did not [@stiller2015ad; @yoon2019role; @katsos2011pragmatic]. 

For the discourse-based category, we selected speaker preference, discourse novelty and discourse continuity tasks. In the speaker preference task, the child had to track the preference of a speaker in order to identify the referent of a novel word [@saylor2009preschoolers]. Discourse novelty refers to a situation in which the child tracks the temporal appearance of objects and expects the speaker to refer to objects that are new in context [@akhtar1996role; @diesendruck2004two]. In the discourse continuity task, the child had to infer and track the topic of an ongoing conversation to resolve ambiguity [@bohn_le_peloquin_koymen_frank_2020; @akhtar2002relevance].

In addition to the pragmatics tasks, we also included two additional cognitive tasks: one measuring executive functions [@zelazo2006dimensional] and the other analogical reasoning [@christie2014language]. Executive functions refer to a family of top-down mental processes that enable us to inhibit automatic or intuitive responses and allow us to concentrate and focus attention on particulars [@diamond2013executive]. A substantial body of research has investigated the link between executive functions and pragmatics -- with mixed results [@matthews2018individual; @nilsen2009relations]. Analogical reasoning refers to the ability to reason about abstract relations between stimuli [@carstensen2021graded] -- an ability that, to our knowledge, has not been specifically linked to pragmatics -- at least not to the same extent as executive functions.

Study 1 and 2 explored the re-test reliability of the pragmatics tasks and found it to be  relatively good. Study 3 tested a larger sample of children to investigate relations between the three utterance-based tasks. We focused on these tasks for theoretical reasons: as noted above, we assume that -- computationally -- they share a common contrastive inference process. We formalize these assumptions in a computational cognitive model which we then use to study individual differences in this alleged process. Study 3 also included tasks for executive functions and analogical reasoning. Across analytical approaches, we found systematic relations among the pragmatics tasks as well as between pragmatics and executive functions, but not analogical reasoning. In the discussion, we use the structure of the cognitive model to speculate about the psychological processes shared between pragmatics and executive functions. 

Taken together, this study introduces a set of tasks that reliably measure individual differences in pragmatic abilities in the preschool years. In addition, it introduces a new (formal) theoretical framework that help us understand individual differences on a process level and, with that, suggests answers to why pragmatic abilities relate to other cognitive abilities.  


# Study 1

Study 1 focused on the psychometric properties of four pragmatics tasks, in particular, their re-test reliability. We chose our sample size so that we would detect medium to high re-test correlations with sufficient power. Two of the tasks were from the utterance-based group and two from the common ground/discourse-based group. This design allowed us to explore whether tasks within one group are more related to one another than between groups. As a fifth task, we included a measure of executive functions. Methods and sample size were pre-registered at https://osf.io/6a723. All analysis scripts and data files can be found in the following repository: https://github.com/manuelbohn/pragBat. The same repository also contains the code to run the experiments. 

## Participants
```{r}
dem <- data%>%
  group_by(id)%>%
  summarise(study = unique(study),
            gender = unique(gender),
            age = unique(age),
            testdays = length(unique(test_day)))%>%
  group_by(study)%>%
  summarise(n = length(id),
            reli_n = sum(testdays == 2),
            female = sum(gender == "f"),
            mage = mean(age),
            lrange = range(age)[1],
            urange = range(age)[2])
```


For Study 1, we collected data from `r dem%>%filter(study == "study1")%>%pull(n)` children ($m_{age}$ = `r dem%>%filter(study == "study1")%>%pull(mage)`, range$_{age}$: `r dem%>%filter(study == "study1")%>%pull(lrange)` - `r dem%>%filter(study == "study1")%>%pull(urange)`, `r dem%>%filter(study == "study1")%>%pull(female)` girls), of whom `r dem%>%filter(study == "study1")%>%pull(reli_n)` were tested twice. For most children, the two test sessions were two days apart; the longest time difference was six days. Children came from an ethnically homogeneous, mid-size German city (~550,000 inhabitants, median income €1,974 per month as of 2020); were mostly monolingual and had mixed socioeconomic backgrounds. The study was approved by an internal ethics committee at the Max Planck Institute for Evolutionary Anthropology. Data was collected between November 2019 and January 2020. 

## Material and Methods

(ref:figlab1) Overview of the tasks used in Study 1 to 3. Pictures show screenshots from each task. The vertical order corresponds to the order of presentation in each study. The colors group the tasks along the (assumed) cognitive processes involved. Blue: utterance-based inferences. Red common ground/discourse-based inferences. Green: Executive functions. Yellow: Analogical reasoning. Bold numbers show the number of trials per task.

```{r fig1, include = T, fig.align = "center", fig.cap = "(ref:figlab1)", out.width="100%"}
knitr::include_graphics("./figures/figure1.png")
```

The study was presented as an interactive picture book on a tablet computer [@frank2016using]. The tasks were programmed in `HTML/JavaScript` and run in a web browser. Pre-recorded sound files were used to address the child (one native German speaker per animal). Children responded by touching objects on the screen. Children were tested in a quiet room in their daycare or in a separate room in a child laboratory. An experimenter guided the child through the study, selecting the different tasks and advancing within each task. In the beginning of the study, children completed a touch training to familiarize themselves with selecting objects. After a short introduction to the different animal characters, children completed the following six tasks. Figure \@ref(fig:fig1) shows screenshots for each task and the order in which they were presented.

### Training

An animal was standing on a pile between two tables. On each table, a familiar object was located. The animal asked the child to give them one of the objects (e.g., "Can you give me the car"). The objects were chosen so that children of the youngest age group would easily understand them (car and ball). This procedure familiarized the child with the general logic of the animals making requests and the child touching objects. There were two training trials. 

### Mutual exclusivity

This task was adapted from @bohn2021young. The task layout and the procedure was the same as in the training. In each trial, one object was a novel object (drawn for the purpose of this study) while the other one was likely to be familiar to children. Both object types changed from trial to trial. Following @bohn2021young, the familiar objects varied in terms of the likelihood that they would be familiar to children in the age range (carrot, duck, eggplant, garlic, horseshoe). For example, we assumed that most 3-year-olds would recognize a carrot, whereas fewer children would recognize a horseshoe. The animal always used a novel non-word (e.g., gepsa) in their request. We reasoned that children would identify the novel object as the referent of the novel word because they assumed the animal would have used the familiar word if they wanted to request the familiar object. Children's response was thus coded as correct if they selected the novel object. There were five trials, with the side on which the novel object appeared pseudo-randomized. 

### Informativeness inference

The task was adapted from @bohn_tessler_merrick_frank_2019. The animal was standing between two trees with objects hanging in them. In one tree, there were two objects (type A and B) and in the other tree there was only one (type B). The animal turned to the tree with the two objects and labeled one of the objects. It was unclear from the animal's utterance, which of the two objects they were referring to. We assumed that children would map the novel word onto the object of type A because they expected the animal to turn to the tree with only the object of type B if their intention was to provide a label for an object of type B. Next, the trees were replaced by new ones, one of which carried an object of type A and the other of type B. The animal then said that one of the trees had the same object as they labeled previously (using the same label) and asked the child to touch the tree. We coded as correct if the child selected the tree with the object of type A. The first two trials were training trials, in which there was only one object in each tree. There were five test trials. The location of the tree with the two objects in the beginning of each trial was pseudo-randomized and so was the location of the objects when the new trees appeared. 

### Speaker preference

This task was also adapted from @bohn_tessler_merrick_frank_2019. The animal was standing between the two tables, each of which had a novel object (drawn for the purpose of the study) on it. The animal turned to one table, pointed at the object and said that they very much liked this object (using a pronoun instead of a label). Next, the animal turned to the other table and said that they really did not like the object (again, using a pronoun and no label). Then the animal turned towards the participant and used a novel label to request an object in an excited tone. We assumed that children would track the animal's preference and identify the previously liked object as the referent. Thus, we coded as correct if the child selected the object the animal expressed preference for. There were five test trials. The location of the preferred object as well as whether the animal first expressed liking or disliking was pseudo-randomized across trials

### Discourse novelty

This task was adapted from @bohn2021young. Once again, the animal was standing between the two tables. One table was empty whereas there was a novel object on the other table. The animal turned towards the empty table and commented on its emptiness. Next, the animal turned to the other table and commented (in a neutral tone) on the presence of the object (not using a label). The animal then briefly disappeared. In the absence of the animal a second novel object appeared on the previously empty table. Then the animal returned and, facing the participant, asked for an object in an excited tone. We assumed that children would track which object was new to the ongoing interaction and identify the object that was new in context as the referent. We coded as correct when children selected the object that appeared later. There were five test trials. The location of the empty table and whether the animal first commented on the presence or absence of an object was pseudo-randomized across trials

### Card sorting

This task was modeled after @zelazo2006dimensional. The child saw two cards, a blue rabbit on the left and a red boat on the right. The experimenter introduced the child to the color game they would be playing next. In this game, all blue cards (irrespective of objects depicted) would go to the left card and all red cards to the right. Next, a third card appeared in the middle of the screen (red rabbit or blue boat) and the experimenter demonstrated the color sorting by moving the card to the one with the same color. After a second demonstration trial, the child started to do the color sorting by themselves. After six trials, the experimenter said that they were now going to play a different game, the shape game, according to which all rabbits would go to the card with the rabbit (left) and all boats to the card with the boat (right). The experimenter repeated these instructions once and without any demonstration the child continued with the sorting according to the new rule. There were six test trials. The shape on the card was pseudo-randomized across trials. We only coded the trials after the rule change and coded as correct when the child sorted according to shape.   

Each child received exactly the same version of each task and completed the tasks in the same order, with the same order on the two days. This ensured comparability of performance across children. 

## Analysis

We analyzed the data in three steps. First we investigated developmental effects in each task, then we assessed re-test reliability, and finally, we looked at relations between the tasks. All analyses were run in `R` [@R-base] version 4.1.2. Regression models were fit as Bayesian generalized linear mixed models (GLMM) using the function `brm` from the package `brms` [@burkner2017brms]. We used default priors for all analysis. 

To estimate developmental effects in each task, we fit a GLMM predicting correct responses (0/1) by age (in years, centered at the mean) and trial number (also centered). The model included random intercepts for each participant and random slopes for trial within participants (model notation in `R`: `correct ~ age + trial + (trial|id)` ). We pre-registered the inclusion of random intercepts for item. We deviate from this here because the order of items was fixed and the same for all participants so that trial and item were confounded for each task. For each task, we inspected and visualized the posterior distribution (mean and 95% Credible Interval (CrI)) for the age estimate.

We assessed re-test reliability in two ways. First, for each task we computed the proportion of correct trials for each individual in the two test sessions and then used Pearson correlations to quantify re-test reliability. Second, we used a GLMM based approach suggested by @rouder2019psychometrics. Here, a GLMM was fitted to the trial-by-trial data for each task with a fixed effect of age (in years, centered at the mean), a random intercept for each participant and a random slope for test day (`correct ~ age + (0+test_day|id)`). The notation `0+test_day` yields a separate intercept estimate for each test day and subject instead of an intercept estimate for day 1 and a slope for the difference between day 1 and day 2. As a consequence, the model estimates the correlation between the two test days instead of a correlation between an intercept and the slope for test day. The correlation between test days can be interpreted as the re-test reliability. This approach has several advantages. First, it uses trial-by-trial data and avoids information loss that comes with data aggregation. Second, it uses hierarchical shrinkage to obtain better participant-specific estimates. Finally, it allows us to get an age-independent estimate for reliability. One worry when assessing re-test reliability in developmental studies is that re-test correlations can be high because of domain general cognitive gains and not because of task-specific individual differences. By including age as a fixed effect in the model, the estimates for each participant are independent of age and so is the correlation between estimates for the two test days -- the re-test reliability.

Finally, we used aggregated data from both test days for each participant and task to compute Pearson correlations between the different tasks. Given the small sample size in Study 1, this part of the analysis was mostly exploratory. 

## Results

(ref:figlab2) Results by task for studies 1 to 3. Each panel shows the results for one task. Regression lines show the predicted developmental trajectories (with 95% CrI) based on by-task GLMMs, with the line type indicating the study. Colored points show age group means (with 95% CI based on non-parametric bootstrap) with the different shapes corresponding to the different studies. Light shapes show the mean performance for each subject by study. Dotted line shows the level of performance expected by chance.

```{r fig2, include = T, fig.align = "center", fig.cap = "(ref:figlab2)", out.width="100%"}
knitr::include_graphics("./figures/figure2.png")
```

We found developmental effects in most of the tasks. Figure \@ref(fig:fig2) shows the data and visualizes the developmental trajectories based on the model. Figure \@ref(fig:fig3) shows the model estimates for age. In the mutual exclusivity task, performance was reliably above chance level and increased with age. For informativeness inference, the pattern was quite different: Performance was at chance level with only minor developmental gains. In the speaker preference task, performance was again clearly above chance with developmental gains resulting in a ceiling effect for older children. In the discourse novelty task, performance was also above chance with no clear developmental effects. The card sorting task showed the strongest developmental effects with younger children performing largely below chance and older children performing above chance. 

Re-test reliability was high for most tasks (see Figure \@ref(fig:fig4)). Raw correlation between the two test sessions was above .7 for mutual exclusivity, speaker preference and discourse novelty, though it was slightly lower for card sorting (.62). The model based -- age independent -- reliability estimates yielded similar results suggesting that the tasks did capture task specific individual differences. A notable exception was the informativeness inference task, which was not reliable according to any of the methods of computing re-test reliability (Figure \@ref(fig:fig4)). We suspected the overall low variation in performance to be responsible for this. 

```{r, echo = F}
cors <- read_csv("../saves/by_task_correlations.csv")
```

Most correlations between the tasks were low and ranged between *r* = -0.2 and 0.2 (see Figure \@ref(fig:fig2)). A notable exception was the correlation between mutual exclusivity and card sorting (*r* = `r cors%>%filter(study == "study1", task1 == "mutual_exclusivity", task2 == "card_sorting")%>%pull(rho)`, 95% CI[`r cors%>%filter(study == "study1", task1 == "mutual_exclusivity", task2 == "card_sorting")%>%pull(lci)` - `r cors%>%filter(study == "study1", task1 == "mutual_exclusivity", task2 == "card_sorting")%>%pull(uci)`]). 

(ref:figlab3) Model estimates (with 95% CrI) for age (in years, centered at the mean) based on GLMMs for each task and study. 

```{r fig3, include = T, fig.align = "center", fig.cap = "(ref:figlab3)", out.width="60%"}
knitr::include_graphics("./figures/figure3.png")
```

(ref:figlab4) Re-test and task correlations for Study 1 (A), 2 (B) and 3 (C). The diagonal in A and B shows the re-test reliability based on aggregated raw test scores (top row) and based on a GLMM that accounted for participant age (see main text for details).

```{r fig4, include = T, fig.align = "center", fig.cap = "(ref:figlab4)", out.width="100%"}
knitr::include_graphics("./figures/figure4.png")
```

## Discussion

Study 1 showed that the different tasks were – for the most part – age appropriate and reliable. A notable exception was the informativeness inference task which generated no systematic variation in the age range we studied here. Correlations between the tasks were generally low, with the exception of the relation between mutual exclusivity and card sorting. Given the small sample size, we avoid overly strong claims, however, it was interesting to see that the relation between the two tasks tapping into discourse-based inferences (speaker preference and discourse novelty) were -- if anything -- negatively correlated.

# Study 2

The goal of Study 2 was to assess the re-test reliability in a new set of tasks. We retained the mutual exclusivity and card sorting tasks because of the interesting relation between the two found in Study 2. We simplified the informativeness inference task to be more age appropriate with the hope of inducing more variation in performance. We removed the speaker preference and discourse novelty tasks -- despite their excellent re-test reliability -- because they seemed to be unrelated to one another and also unrelated to the other tasks. We also added new tasks focused on ad-hoc implicature and discourse continuity. As noted in the introduction, we had theoretical reasons to expect the ad-hoc implicature task to be related to the mutual exclusivity and informativeness inference tasks. We had no such strong predictions for the discourse continuity task.

Methods and sample size were pre-registered at https://osf.io/hp9f7. Data, analysis scripts and experiment code can be found in the associated online repository. 

## Participants

Participants for Study 2 were recruited from the same general population. We collected data from `r dem%>%filter(study == "study2")%>%pull(n)` children ($m_{age}$ = `r dem%>%filter(study == "study2")%>%pull(mage)`, range$_{age}$: `r dem%>%filter(study == "study2")%>%pull(lrange)` - `r dem%>%filter(study == "study2")%>%pull(urange)`, `r dem%>%filter(study == "study2")%>%pull(female)` girls), of whom `r dem%>%filter(study == "study2")%>%pull(reli_n)` were tested twice. The two test sessions were again two days apart; the longest time difference was 14 days. Data was collected between March and October 2020.

## Material and Methods

The general setup and mode of presentation was the same as in Study 1. We added two new tasks and modified the informativeness inferences task, which we will describe in detail below. The training, mutual exclusivity and card sorting tasks were the same as in Study 1. 

### Informativeness inference

The general structure of the task was the same as in Study 1, however, we replaced the stimuli from @bohn_tessler_merrick_frank_2019 with those used originally by @frank2014inferring. We suspected that many children did not treat the novel objects hanging in the tree as properties of the tree but rather viewed the tree as a “container” for the novel objects. The alleged inference, however, relies on seeing the objects as properties of the referent. With the new stimuli, we emphasized that the novel objects were mere properties of the referent by making the referent more salient and more different across trials. The animal was located between two identical objects, which had different properties (see Figure \@ref(fig:fig1)). For example, the child saw two bears, one with a Pharaoh-style crown and a match in its hand, the other only with the match. The animal then turned to the object with the two properties and described it by referring to one of the properties (e.g., a bear with a [non-word]). Next, the objects disappeared, and the same objects re-appeared but this time, each of them had only one property (e.g., one bear with a crown, the other with the match). The animal then asked which of these objects had the aforementioned property (e.g., which bear has a [non-word]). We coded responses as correct if the child selected the object with the property that was unique to the object during labeling. The first two trials were training trials, in which each object only had one property. There were five test trials. The location of the object with the two properties in the beginning of each trial was pseudo-randomized and so was the location of the properties when the new objects appeared. 

### Discourse continuity

This task was adapted from @bohn_le_peloquin_koymen_frank_2020. Children were told that they were going to visit the animals in their home. An animal greeted the child and told them that they would show them their things. During exposure trials, the child saw three objects from three different categories (e.g., train (vehicle), drum (instrument), orange (fruit); see Figure \@ref(fig:fig1)). The animal named one of the objects and asked the child to touch it. On the next exposure trial, the child saw three new objects but from the same categories (e.g., bus (vehicle), flute (instrument), apple (fruit)). The animal asked the child to touch the object from the same category as previously (only naming the object, not the category). There were 5 such exposure trials. On the following test trial, the animal used a pronoun to refer to one of the objects (i.e., can you touch *it*). We assumed that children would use the exposure trials to infer that the animal was talking about a certain category and would use this knowledge to identify the referent of the pronoun. Children received five test trials, each with a different category as the target. The position of the objects in exposure trials as well as test trials was pseudo-randomized.  

### Ad-hoc implicature

This task used the general procedure and stimuli developed in @yoon2019role. The animal was located in a window, looking out over two objects (see Figure \@ref(fig:fig1)). Both objects were of the same kind, but had different properties. As properties we chose objects that were well known to children of that age range. One object had one property (A), while the other had two (A and B). For example, objects were lunch boxes, one with an orange and the other with an orange and an apple. The animal then asked the child to hand them their object which was the one with the property that both objects shared (A). We assumed that children would pick the object with only property A because they expected the animal to name property B if they had wanted to refer to the object with both properties. There were five test trials, preceded by two training trials in which the objects did not share a common property. The positioning of the objects (left and right) was pseudo-randomized.

## Analysis

We used the same methods to analyze the data as in Study 1. 

## Results

We found substantial developmental gains in all five tasks (Figure \@ref(fig:fig2) and \@ref(fig:fig3)). For mutual exclusivity and ad-hoc implicature performance was above chance across the entire age range. For the informativeness inference and discourse continuity tasks, performance was close to chance for younger children and reliably above it for older children. Like in Study 1, we found the strongest developmental effect for card sorting, with performance below chance for 3-year-olds and above chance for 4-year-olds. 

Re-test reliability based on aggregated data was good for all tasks with most estimates around 0.7. The model-based reliability estimates were similar, with lower values for ad-hoc implicature and higher ones for discourse continuity. Notably, the revised informativeness inference task showed a much-improved re-test reliability compared with the estimate from Study 1. 

Correlations between tasks were generally higher compared to Study 1. In fact, confidence intervals for correlation coefficients were not overlapping with 0 except for the correlation between the discourse continuity and informativeness inference tasks (Figures \@ref(fig:fig4). Once again, we found the strongest relation between card sorting and mutual exclusivity (*r* = `r cors%>%filter(study == "study2", task1 == "mutual_exclusivity", task2 == "card_sorting")%>%pull(rho)`, 95% CI[`r cors%>%filter(study == "study2", task1 == "mutual_exclusivity", task2 == "card_sorting")%>%pull(lci)` - `r cors%>%filter(study == "study2", task1 == "mutual_exclusivity", task2 == "card_sorting")%>%pull(uci)`]). Other notable relations were those between card sorting and informativeness inference (*r* = `r cors%>%filter(study == "study2", task1 == "simple_inf", task2 == "card_sorting")%>%pull(rho)`, 95% CI[`r cors%>%filter(study == "study2", task1 == "simple_inf", task2 == "card_sorting")%>%pull(lci)` - `r cors%>%filter(study == "study2", task1 == "simple_inf", task2 == "card_sorting")%>%pull(uci)`]) as well as between ad-hoc implicature and informativeness inference (*r* = `r cors%>%filter(study == "study2", task1 == "simple_inf", task2 == "ad_hoc_implicature")%>%pull(rho)`, 95% CI[`r cors%>%filter(study == "study2", task1 == "simple_inf", task2 == "ad_hoc_implicature")%>%pull(lci)` - `r cors%>%filter(study == "study2", task1 == "simple_inf", task2 == "ad_hoc_implicature")%>%pull(uci)`]). 

## Discussion

In Study 2 we found good results from a measurement perspective: all tasks had acceptable re-test reliability. This result extended to the informativeness inference task, which had very low reliability in Study 1. Higher average performance and increased variability both suggest that our changes to the stimuli made the task easier for children.

As in Study 1, we found a relatively strong correlation between the mutual exclusivity and card sorting tasks. This finding supports the idea that these tasks share common processes. We also found substantial relations between the three utterance-based inference tasks (mutual exclusivity, ad-hoc implicature, informativeness inference). The correlations between these tasks and the discourse continuity task were numerically lower.

# Study 3

In Study 3, we focused explicitly on the relations between the different tasks. In particular, we explored the idea that the three utterance-based inference tasks share common cognitive processes. Once again, we also included the card sorting task and added a new task of analogical reasoning as a control for which we did not expect strong relations with the other tasks. To be able to test predictions about cross-task variation, we collected data from a comparatively larger sample of children.

The reliability estimates from Study 1 and 2 helped us plan the sample size for Study 3. The focal tasks had a re-test reliability around 0.7. Because the highest plausible correlation between two tasks is the product of their reliabilities (higher correlations would mean that the task is more strongly related to a different task than to itself), the highest we could expect were correlations between two tasks around 0.7 * 0.7 = 0.49. We planned our sample so that we could detect correlations between two tasks of 0.3 with 95% power. The first author drafted a pre-registration and shared it with the last author but forgot to register it at OSF. Thus, the study was not officially pre-registered. Data, analysis scripts and experiment code can be found in the associated online repository. 

## Participants

For Study 3, we collected data from `r dem%>%filter(study == "study3")%>%pull(n)` children ($m_{age}$ = `r dem%>%filter(study == "study3")%>%pull(mage)`, range$_{age}$: `r dem%>%filter(study == "study3")%>%pull(lrange)` - `r dem%>%filter(study == "study3")%>%pull(urange)`, `r dem%>%filter(study == "study3")%>%pull(female)` girls) from the same general population. Data was collected between June and November 2021. Children were tested only once. 

## Materials and Methods

From Study 2, we used the mutual exclusivity, ad-hoc implicature, informativeness inference and card sorting tasks. We added the relational match-to-sample task, which we now describe in more detail. 

### Relational match-to-sample

The task was modeled after (and used the original stimuli from) @christie2014language. The child saw three cards, one on top (the sample) and two at the bottom (the potential matches; see Figure \@ref(fig:fig1)). The experimenter guided the child through the study and read out the instructions. The child was instructed to match the sample card to one of the lower ones based on similarity, that is, they were instructed to pick the card that was "like" the sample. All cards had two geometrical shapes of the same color on them. The sample card showed two identical shapes and so did one of the potential matches. The other card showed two different shapes. We assumed that children would match the sample to the match that showed the same relation between shapes (sameness). Children received six test trials, preceded by two training trials in which one of the potential matches was identical to the sample. The position of the same-match was pseudo randomized.  

## Analysis

Study 3 had only one test session. Therefore, we did not investigate re-test reliability. We estimated age effects and raw correlations between tasks in the same way as in Studies 1 and 2. We used two additional methods to investigate the structure of individual differences between tasks. 

First, we used Confirmatory Factor Analysis (CFA). Models were fit in a Bayesian framework using the `R` package `blavaan` [@merkle2018blavaan] using default priors. As outlined above, our focal model assumed that mutual exclusivity, ad-hoc implicature and informativeness inference load on a common pragmatics factor. The card sorting and relational match-to-sample tasks were included as separate factors. We used Posterior Predictive P-Values (PPP) to evaluate model fit [@lee2012basic]. A good model fit is indicated by a PPP close to 0.5 and should not be smaller than 0.1 [@cain2019fit]. We also fit two alternative models: one including only a single factor on which all tasks loaded and a second with a separate factor for each task. We compared models using WAIC (widely applicable information criterion) scores and weights [@mcelreath2018statistical]. WAIC is an indicator of out-of-sample predictive accuracy with lower values indicating better fit. WAIC weights transform WAIC values to give the probability that a particular model (out of the models considered) provides the best out-of-sample predictions. Within the focal model, we inspected the posterior estimates (with 95%CrI) for the factor loadings and the variance in the task explained by the factor for the three pragmatics tasks. In addition, we evaluated the correlations between the pragmatics factor and the other two tasks.

Second, we used computational cognitive models from the Rational Speech Act (RSA) framework to relate the three pragmatics tasks to one another [@frank2012predicting; @goodman2016pragmatic]. In contrast to the CFA model above, the RSA models are models of the tasks, and not of the data. That is, they include a schematic representation of the experimental tasks and provide a computational account of how participants make inferences in this context. RSA models see pragmatic inferences as a form of Bayesian social reasoning where the listener tries to infer the speaker's meaning (here: the intended referent) by assuming that the speaker is helpful and informative. Being helpful and informative means that the speaker chooses a message based on the probability that it would help the listener to recover the speaker's intended meaning (i.e., select the intended referent). Thus, RSA models have a recursive structure in which the the listener reasons about a speaker who is reasoning about the listener. To avoid an infinite regress, the speaker is assumed to reason about a literal listener, who interprets utterances according to their literal semantics. 


The studies from which we took the mutual exclusivity and informativeness inference tasks also formalized these tasks in an RSA-style model [@bohn_tessler_merrick_frank_2019; @bohn2021young]. We refer to this earlier work for a more detailed description of the models. For the present study, we formalized the ad-hoc implicature task within the same RSA framework. The common model structure is formally defined as: 

$$P_{L_1}(r|u)\propto P_{S_1}(u|r) \cdot P(r)$$
In the above equation, the listener ($P_{L_1}$) is trying to infer the speaker’s ($P_{S_1}$) intended referent $r$ by imagining what a rational speaker would say, given the referent they are trying to communicate and the listener’s prior expectations about the referent $P(r)$ (which we assumed to be uniform over potential referents). The speaker is an approximately rational Bayesian actor (with degree of rationality $\alpha$) who produces utterances as a function of their informativity. 

$$P_{S_1}(u|r)\propto Informativity(u;r)^\alpha$$
The informativity of an utterance for a referent is taken to be the probability with which a naive listener ($P_{L_0}$), who only interprets utterances according to their literal semantics, would select a particular referent given an utterance.

$$Informativity(u; r) = P_{L_0}(r|u)$$

The three models differ in the types of utterances that are being produced, however, they share the same contrastive inference process according to which the listener ($P_{L_1}$) compares the speaker’s ($P_{S_1}$) utterance to a set of alternative, possible utterances. As noted above, the listener expects the speaker to be informative (with degree $\alpha$) that is, choose the utterance that best communicates the intended message. In the mutual exclusivity task, the speaker produced an unfamiliar word; thus, the alternative utterance for the speaker would have been to use a familiar word. In the case of the informative inference task, the speaker pointed to the object with two properties; thus, the alternative would have been to point to the object with only one property. For the ad-hoc implicature task the speaker referred to the property shared by the two objects, which contrasts with referring to the property that was unique to one of the objects. In all cases, these alternative utterances would be better suited to communicate about the respective other referent.

As noted above, models for the different tasks shared one common parameter: the speaker informativeness parameter $\alpha$. This commonality offers a way of relating performance in the three tasks to one another by constraining the three models to use the same value for $\alpha$. We then used Bayesian inference to estimate the posterior distribution for $\alpha$ that best explained performance in the three tasks. To adapt this framework to the study of individual differences, we allowed a separate parameter for each participant ($\alpha_i$). We estimated $\alpha_i$ in a hierarchical model as a deviation from a hyper parameter: $\alpha_i \sim \mathcal{N}(\alpha_j, \sigma^\alpha)$. Given the developmental nature of our data, we defined $\alpha_j$ via a linear regression as a function of the child's age ($age_i$): $\alpha_j = \beta^\alpha_0 + age_i \cdot \beta^\alpha_1$. Thus, the participant-specific value for $\alpha$ was not only constrained by the performance in the three tasks but also by the child's age.

To account for differences in difficulty between the tasks due to other factors, we added a scale parameter to the model that adjusted $\alpha$ for each task in comparison to a reference task (ad-hoc implicature).

To validate this approach, we first applied this model to the data from Study 2 separately for each test session. This allowed us to compute the re-test reliability of $\alpha$ and see if it captures individual differences equally well compared to the raw test scores. After finding excellent re-test reliability, we applied it to the data from Study 3 and correlated the results with the card sorting and relational match-to sample tasks. For this correlational analysis, we converted the posterior distribution for each participant into a single value by taking the mode (and 95% highest density interval -- HDI). The cognitive models were implemented in `WebPPL` [@dippl] and the corresponding code, including information on prior distributions (which we omit here for space), can be found in the associated online repository. 

## Results

The age effects in Study 3 largely replicate those of Study 2 for the four overlapping tasks (see Figure \@ref(fig:fig2) and \@ref(fig:fig3)). There were no substantial developmental gains in the newly added relational match-to-sample task and performance was close to chance for both age groups. Thus -- in the absence of information on re-test reliability -- it is unclear if the variation in performance reflects systematic individual differences in analogical reasoning or not.

Overall, the correlations between the tasks were lower compared to Study 2. This was to some extent expected given that there were only half the number of trials per task in Study 3 and, hence less "signal" (systematic, non-error variability) for capturing individual differences. Nevertheless, the overall pattern resembles that found in Study 2 (Figure \@ref(fig:fig4)). We saw the strongest bi-variate relation between the mutual exclusivity and the ad-hoc implicature task (*r* = `r cors%>%filter(study == "study3", task1 == "mutual_exclusivity", task2 == "ad_hoc_implicature")%>%pull(rho)`, 95% CI[`r cors%>%filter(study == "study3", task1 == "mutual_exclusivity", task2 == "ad_hoc_implicature")%>%pull(lci)` - `r cors%>%filter(study == "study3", task1 == "mutual_exclusivity", task2 == "ad_hoc_implicature")%>%pull(uci)`]) followed by ad-hoc implicature and card sorting (*r* = `r cors%>%filter(study == "study3", task1 == "card_sorting", task2 == "ad_hoc_implicature")%>%pull(rho)`, 95% CI[`r cors%>%filter(study == "study3", task1 == "card_sorting", task2 == "ad_hoc_implicature")%>%pull(lci)` - `r cors%>%filter(study == "study3", task1 == "card_sorting", task2 == "ad_hoc_implicature")%>%pull(uci)`]). The relational match-to-sample task showed no substantial correlations with any of the other tasks. 

```{r}
model_comparison <- read_csv("../saves/bcfa_model_comparison.csv")

model_fit <- read_csv("../saves/cfa_fit_measures.csv")

model_est <- read_csv("../saves/cfa_estimates.csv")

```

(ref:figlab5) Graphical overview of CFA model for Study 3. Arrows from latent variable (circles) to observed variable (rectangles) show factor loadings. Bottom arrows to observed variables give the residual variance not explained by the factor. Bent arrows between latent variables show correlations. Bottom rows show model estimates with 95% CrI. Top rows show standardized estimates (bold if 95 % CrI does not include 0). 

```{r fig5, include = T, fig.align = "center", fig.cap = "(ref:figlab5)", out.width="100%"}
knitr::include_graphics("./figures/figure5.png")
```

Next, we turn to the results of the confirmatory factor analysis. Our focal model -- including a latent factor for pragmatic reasoning -- fit the data well (PPP = `r round(model_fit%>%filter(index == "ppp", model == "fit1")%>%pull(value),3)`) and with a WAIC of `r round(model_comparison%>%filter(model == "fit1")%>%pull(waic),2)` (se = `r round(model_comparison%>%filter(model == "fit1")%>%pull(waic_se),2)`, weight = `r round(model_comparison%>%filter(model == "fit1")%>%pull(waic_weight),2)`) better compared to the two alternative models (individual factors model: PPP = `r round(model_fit%>%filter(index == "ppp", model == "fit3")%>%pull(value),3)`, WAIC = `r round(model_comparison%>%filter(model == "fit3")%>%pull(waic),2)`, se = `r round(model_comparison%>%filter(model == "fit3")%>%pull(waic_se),2)`, weight = `r round(model_comparison%>%filter(model == "fit3")%>%pull(waic_weight),2)`; one factor model: PPP = `r round(model_fit%>%filter(index == "ppp", model == "fit2")%>%pull(value),3)`, WAIC = `r round(model_comparison%>%filter(model == "fit2")%>%pull(waic),2)`, se = `r round(model_comparison%>%filter(model == "fit2")%>%pull(waic_se),2)`, weight = `r round(model_comparison%>%filter(model == "fit2")%>%pull(waic_weight),2)`). 

Figure \@ref(fig:fig5) shows factor loadings for the individual tasks as well as their residual variance. The latent pragmatic reasoning factor best explained the mutual exclusivity task, followed by the ad-hoc implicature and the informativeness inference task. The correlation between pragmatic reasoning and executive functions (indicated by the card sorting task) was estimated to be reliably different from zero (*r* = 0.33; model estimate = `r round(model_est%>%filter(index == "prag~~card")%>%pull(value),2)`, 95% CrI [`r round(model_est%>%filter(index == "prag~~card")%>%pull(lower),2)` - `r round(model_est%>%filter(index == "prag~~card")%>%pull(upper),2)`]). There was no systematic relation between pragmatic reasoning and analogical reasoning (as indicated by the relational match-to-sample task): *r* = 0.06; model estimate = `r round(model_est%>%filter(index == "prag~~match")%>%pull(value),2)`, 95% CrI [`r round(model_est%>%filter(index == "prag~~match")%>%pull(lower),2)` - `r round(model_est%>%filter(index == "prag~~match")%>%pull(upper),2)`]. However, the latter result should be taken with a grain of salt given the unknown psychometric properties of the relational match-to-sample task.

```{r}

rsa_cor <- read_csv("../saves/rsa_cor_estimates.csv")

```

Finally, we present the results of the cognitive modeling analysis. Using the data from Study 2, we saw that participant specific speaker informativeness parameters ($\alpha$) were highly reliable (Figure \@ref(fig:fig6)B). The scale parameter suggested that the mutual exclusivity task was easier and the informativeness inference task was harder compared to the ad-hoc implicature task (Figure \@ref(fig:fig6)C). When correlating $\alpha$ with performance in the other two tasks, the cognitive modeling approach yielded similar conclusions compared to the confirmatory factor analysis (Figure \@ref(fig:fig6)A): There was a substantial correlation with the card sorting (*r* = `r rsa_cor%>%filter(task == "Card Sorting")%>%pull(cor)`, 95% CI[`r rsa_cor%>%filter(task == "Card Sorting")%>%pull(lci)` - `r rsa_cor%>%filter(task == "Card Sorting")%>%pull(uci)`]) but not the relational match-to-sample task (*r* = `r rsa_cor%>%filter(task == "Relational Match-to-sample")%>%pull(cor)`, 95% CI[`r rsa_cor%>%filter(task == "Relational Match-to-sample")%>%pull(lci)` - `r rsa_cor%>%filter(task == "Relational Match-to-sample")%>%pull(uci)`]). The same limitations apply to the latter result as for the confirmatory factor analysis. 

(ref:figlab6) Results of cognitive model analyses. A: Correlation between the speaker informativeness parameter $\alpha$ and the performance in the card sorting and relational match to sample tasks. Regression line (with 95% CI) is based on a linear model. B: Re-test reliability for $\alpha$ based on the data from Study 2. C: Scale parameter for $\alpha$ in relation to the ad-hoc implicature task. Values below 1 indicate a more difficult task, values above 1 an easier task. Correlation coefficients show Pearson correlation with 95% CI.

```{r fig6, include = T, fig.align = "center", fig.cap = "(ref:figlab6)", out.width="100%"}
knitr::include_graphics("./figures/figure6.png")
```

## Discussion

Using a diversity of analytical tools, we found that performance in the three utterance-based pragmatic inference tasks was related in a way that points to shared cognitive processes. In the confirmatory factor analysis, we found that a model including a latent pragmatic reasoning factor fit the data well and better compared to alternative models. The latent factor explained substantial portions of the variance in each of the three tasks. The cognitive modeling approach provides an explicit theory of what the shared cognitive processes may look like: according to the model, the pragmatic inference in each task was driven by contrasting the utterance the speaker produced and alternative utterances. Individual differences were thought to arise from differential expectations about how informative the speaker is. 

Both analytic strategies point to systematic relations between pragmatic reasoning and executive functions as indicated by the card sorting test. We found no such relations with analogical reasoning as indicated by the relational match-to-sample task. However, given the unknown psychometric properties of the latter task, this result should be interpreted with caution. 

# General Discussion

In this paper, we explored the development of pragmatic inferences in the preschool years. We identified six tasks covering a broad range of pragmatic phenomena. We found them to have generally good re-test reliability. We then selected three utterance-based inference tasks for a well-powered study of relations among different types of pragmatic abilities and between pragmatics and other cognitive abilities. The results showed systematic relations between the utterance-based tasks, consistent with a latent cognitive construct. We used a computational cognitive model of pragmatic reasoning to formalize the cognitive processes we believed the tasks to share. Finally, we found pragmatic abilities to be related to a task of executive functions

One of the main contributions of this paper is that it presents six pragmatic inference tasks that are highly robust and reliable. Whenever we used a task in two studies (mutual exclusivity, informativeness inference, ad-hoc implicature), we found developmental results that replicated previous findings. In Study 1 and 2, all tasks showed good re-test reliability – even when corrected for age. A notable exception was the informativeness inference task in Study 1. However, after making some procedural changes, it turned out to be robust and reliable as well. Taken together, the tasks are suitable for individual differences research, advancing the agenda of @matthews2018individual. These materials are freely available via the associated online repository.

We grouped our pragmatics tasks into utterance-based and common ground/discourse based. This grouping broadly captured the kind of information that we assumed to be relevant to compute the inference. For Study 3, we focused on the three utterance-based tasks. The main reason was theoretical. We were able to build on earlier work [@bohn_tessler_merrick_frank_2019; @bohn2021young] and formalize the inferences involved in these tasks in a common computational framework. We specified the structural overlap between the tasks and identified a parameter in the model that we used to capture individual differences. The shared structural features involve a recursive social inference process according to which the listener expects the speaker to select the most informative of a set of possible utterances. The individual difference parameter captured how informative the listener expected the speaker to be. Previous accounts would not have predicted such an overlap. In particular, theoretical accounts of mutual exclusivity as arising from heuristics or principles unconnected with pragmatic reasoning [reviewed in @lewis2020role] do not make the prediction of correlations with other pragmatic tasks. 

Our formal model also allowed us to speculate about why we saw a systematic relation across the three studies between pragmatic inference and the card sorting task as a measure of executive functions. Before we do so, we want to emphasize that the model is first and foremost a computational description of the tasks and not a model of a psychological process [cf. @goodman2016pragmatic]. Here we speculate, assuming a bit more psychological realism in our interpretation of the RSA model than previous authors have. The card sorting task asks the child to switch between rules after having practiced the first rule over the course of several trials. This switch requires inhibiting a pre-potent response and attending to different features of the cards. Similarly, pragmatic inference in the RSA model involves contrasting the observed utterance with alternative plausible utterances. This process, too, could be described as requiring inhibiting available, plausible interpretations and contrasting different interpretations before making a response. To pursue this connection further, the next step should be to model card sorting and the pragmatics tasks jointly to substantiate such a verbal analysis.

## Limitations

The studies we presented here have important limitations. Our focus on the utterance-based pragmatic inference tasks meant that we did not study or analyze the common ground/discourse-based tasks with the same level of detail. That is, we did not formalize them in a cognitive model and did not study relations between them in a larger sample. Future research should address these shortcomings. Nevertheless, the work presented here is an important first step because it showed that the common ground/discourse tasks themselves have good psychometric properties and are therefore suitable for individual differences research. 

We presented the tasks as interactive picture books on a tablet computer with animal characters as agents. This methodological step improved the quality of our measurement because it allowed us to experimentally isolate the different inferences and run multiple trials in each task. However, it also means that – like most experimental work – our tasks lack ecological validity. In real-world conversations, multiple information sources are available to listeners to draw inferences from [@bohn2021young; @bohn_tessler_merrick_frank_2019]. Furthermore, by design our experimental paradigm prevented the use of strategies that are an integral part of real-world conversations, like asking questions or seeking clarification [@arkel2020simple; @clark1991grounding]. However, we want to highlight that the results from our tasks replicated many findings from interactive versions of these tasks [@markman1988children; @akhtar1996role; @saylor2009preschoolers; @frank2014inferring].

Finally, we only studied one sample of children from a Western, affluent setting. Thus, it is unclear if and how the results would transfer to other settings [@nielsen2017persistent]. The tasks used here were largely developed and tested with English-speaking children in the US. The fact that they transferred well to the German setting of the current studies is at least a small hint that they might also be suitable to study pragmatic inference in other cultural and linguistic settings. Future research will hopefully test whether that is the case. 

## Conclusion

The studies reported here addressed some fundamental challenges in the study of individual differences in pragmatic abilities [@matthews2018individual]. We developed and validated new methodological and theoretical tools that helped to study the relations between different types of pragmatic inferences as well as between pragmatics and other cognitive abilities in a more reliable and valid way. This approach emphasizes the interdependent nature of theoretical and methodological progress and provides a roadmap for future work. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
